Below is the complete, updated role structure and file contents for upgrade_websphere. In this version, the stop and start handlers include built‐in validation steps. That is, after issuing the stop (or start) commands, each handler runs additional tasks to verify that the service is indeed down (or up) and fails if the expected state is not reached.

All paths are variable‐based using
{{ websphere_install_root }}/profiles/*/bin
for WebSphere commands and
{{ ihs_install_root }} along with ihs_instance for IBM HTTP Server (IHS) commands.

The role runs only on hosts with is_appserver: true. DMGR tasks run only when is_dmgr: true and IHS tasks run only when is_ihs: true.

Below you’ll find:

A summary of features and flow
The directory structure
Complete file contents with inline explanations
1. Features & Flow
Variable-Based Paths:
All commands use variables (for example, {{ websphere_install_root }} and {{ ihs_install_root }}) so that you can change installation roots via inventory or group variables.

Conditional Execution:
• The role executes on hosts where is_appserver is true.
• DMGR tasks are executed only on hosts where is_dmgr is true.
• IHS tasks run only when is_ihs is true.

Service Stop/Start Order:
Stop Phase:

Application servers are gathered and stopped (in reverse order: App, then Sup, then ME).
The NodeAgent is stopped next.
On DMGR hosts, the Deployment Manager is stopped.
On IHS hosts, the IBM HTTP Server is stopped.
After issuing the stop commands, each handler validates that the service is really down.
Start Phase:
On DMGR hosts, the Deployment Manager is started first (if stopped).
The NodeAgent is started next.
Application servers are started (ordered as ME, then Sup, then App).
On IHS hosts, the IBM HTTP Server is started.
After issuing the start commands, each handler validates that the service is up.
Validation:
Each handler file contains additional shell tasks that run after the stop (or start) commands to verify the current status using either the WebSphere status script or a ps command for IHS. If validation fails, the handler uses the fail module to stop the play.

Modularity:
All stop/start logic is broken into individual handler sub‑task files. (Two extra handlers for IHS status checking are defined but not called by default.)
The main tasks file simply notifies the handlers in the proper order.

2. Directory Structure
css
Copy
roles/
└── upgrade_websphere
    ├── defaults
    │   └── main.yml
    ├── tasks
    │   └── main.yml
    └── handlers
        ├── main.yml
        ├── stop_appservers.yml
        ├── stop_nodeagent.yml
        ├── stop_dmgr.yml
        ├── stop_httpserver.yml
        ├── start_dmgr.yml
        ├── start_nodeagent.yml
        ├── start_appservers.yml
        ├── start_httpserver.yml
        ├── check_httpserver_status.yml
        └── start_httpserver_status.yml
3. File Contents
A. defaults/main.yml
yaml
Copy
---
# roles/upgrade_websphere/defaults/main.yml

# Default kill timeout in seconds (e.g., 1200 = 20 minutes)
kill_timeout: 1200

# Optional filters for application servers; leave empty to operate on all.
target_server: ""
target_group: ""
B. tasks/main.yml
yaml
Copy
---
# roles/upgrade_websphere/tasks/main.yml

- block:
    - name: Stop all services
      debug:
        msg: "Notifying handlers to stop application servers, NodeAgent and DMGR..."
      notify:
        - stop_appservers
        - stop_nodeagent
        - stop_dmgr

    - name: Stop IBM HTTP Server
      debug:
        msg: "Notifying handler to stop IBM HTTP Server..."
      notify:
        - stop_httpserver
      when: is_ihs | bool

    - name: Perform upgrade and validation tasks
      debug:
        msg: "Performing upgrade tasks and validations..."
      # Replace this with your actual upgrade/validation commands

    - name: Start all services
      debug:
        msg: "Notifying handlers to start DMGR, NodeAgent and application servers..."
      notify:
        - start_dmgr
        - start_nodeagent
        - start_appservers

    - name: Start IBM HTTP Server
      debug:
        msg: "Notifying handler to start IBM HTTP Server..."
      notify:
        - start_httpserver
      when: is_ihs | bool
  when: is_appserver | bool
C. handlers/main.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/main.yml

- import_tasks: stop_appservers.yml
- import_tasks: stop_nodeagent.yml
- import_tasks: stop_dmgr.yml
- import_tasks: stop_httpserver.yml
- import_tasks: start_dmgr.yml
- import_tasks: start_nodeagent.yml
- import_tasks: start_appservers.yml
- import_tasks: start_httpserver.yml
- import_tasks: check_httpserver_status.yml
- import_tasks: start_httpserver_status.yml
D. handlers/stop_appservers.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/stop_appservers.yml

- name: stop_appservers
  block:
    - name: Gather running application servers for stop
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: ws_status_stop
      when: (target_server | default('')) == "" and (target_group | default('')) == ""

    - name: Set regex for running application servers (to stop)
      set_fact:
        status_regex: "is STARTED"
      when: (target_server | default('')) == "" and (target_group | default('')) == ""

    - name: Filter lines matching running application servers
      set_fact:
        target_lines: "{{ ws_status_stop.stdout_lines | select('search', status_regex) | list }}"
      when: (target_server | default('')) == "" and (target_group | default('')) == ""

    - name: Build list of running application server names
      set_fact:
        target_servers: >-
          {{
            target_lines
            | map('regex_search', 'The Application Server \"([^\"]+)\"', '\\1')
            | list
          }}
      when: (target_server | default('')) == "" and (target_group | default('')) == ""

    - name: Order application servers for stop (reverse order: App, then Sup, then ME)
      set_fact:
        sorted_servers: >-
          {{
            (target_servers | select('match', '^AppClusterMember') | list) +
            (target_servers | select('match', '^SupClusterMember') | list) +
            (target_servers | select('match', '^MEClusterMember') | list)
          }}
      when: (target_server | default('')) == "" and (target_group | default('')) == ""

    - name: Set sorted_servers for individual target application server
      set_fact:
        sorted_servers: [ "{{ target_server }}" ]
      when: (target_server | default('')) != ""

    - name: Filter sorted_servers by target group if defined (application servers)
      set_fact:
        sorted_servers: "{{ sorted_servers | select('match', '^' ~ target_group ~ 'ClusterMember') | list }}"
      when: (target_group | default('')) != ""

    - name: Stop application servers
      shell: |
        {{ websphere_install_root }}/profiles/*/bin/stopServer.sh {{ server }}
        counter=0
        while ps -ef | grep -v grep | grep -q "{{ server }}"; do
          sleep 10
          counter=$((counter+10))
          if [ $counter -ge {{ kill_timeout }} ]; then
            echo "Application server {{ server }} still running after {{ kill_timeout }} seconds. Killing process..."
            ps -ef | grep -v grep | grep "{{ server }}" | awk '{print $2}' | xargs kill -9 || true
            break
          fi
        done
      args:
        executable: /bin/bash
      loop: "{{ sorted_servers }}"
      loop_control:
        loop_var: server
      when: sorted_servers is defined and (sorted_servers | length > 0)

    - name: Validate application servers are stopped
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: validate_stop_appservers
      changed_when: false
    - name: Fail if any application server is still running
      fail:
        msg: "Application servers are still running: {{ validate_stop_appservers.stdout }}"
      when: "'is STARTED' in validate_stop_appservers.stdout"
After stopping the application servers, this handler validates that none remain running.

E. handlers/stop_nodeagent.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/stop_nodeagent.yml

- name: stop_nodeagent
  block:
    - name: Stop the NodeAgent
      shell: "{{ websphere_install_root }}/profiles/*/bin/stopNode.sh"
      args:
        executable: /bin/bash
    - name: Validate NodeAgent is stopped
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all | grep -i nodeagent"
      register: validate_stop_nodeagent
      changed_when: false
    - name: Fail if NodeAgent is still running
      fail:
        msg: "NodeAgent is still running: {{ validate_stop_nodeagent.stdout }}"
      when: validate_stop_nodeagent.stdout is defined and ("is STARTED" in validate_stop_nodeagent.stdout)
This handler stops the NodeAgent and then checks its status.

F. handlers/stop_dmgr.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/stop_dmgr.yml

- name: stop_dmgr
  block:
    - name: Gather Deployment Manager status for stop
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: dmgr_status_stop
      when: is_dmgr | bool
    - name: Check if Deployment Manager is running (for stop)
      set_fact:
        dmgr_running: "{{ (dmgr_status_stop.stdout_lines | select('search', 'The Deployment Manager \"dmgr\" is STARTED') | list | length) > 0 }}"
      when: is_dmgr | bool
    - name: Stop the Deployment Manager
      shell: "{{ websphere_install_root }}/profiles/*/bin/stopManager.sh"
      args:
        executable: /bin/bash
      when: is_dmgr | bool and dmgr_running
    - name: Validate DMGR is stopped
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: validate_stop_dmgr
      changed_when: false
      when: is_dmgr | bool
    - name: Fail if DMGR is still running
      fail:
        msg: "DMGR is still running: {{ validate_stop_dmgr.stdout }}"
      when: is_dmgr | bool and ("The Deployment Manager \"dmgr\" is STARTED" in validate_stop_dmgr.stdout)
Stops DMGR (if applicable) and validates that it is down.

G. handlers/stop_httpserver.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/stop_httpserver.yml

- name: stop_httpserver
  block:
    - name: Stop IBM HTTP Server
      shell: "{{ ihs_install_root }}/bin/apachectl -d {{ ihs_install_root }} -f {{ ihs_install_root }}/conf/{{ ihs_instance }} -k stop"
      args:
        executable: /bin/bash
    - name: Validate IBM HTTP Server is stopped
      shell: "ps -ef | grep httpd | grep '{{ ihs_instance }}'"
      register: validate_stop_ihs
      changed_when: false
    - name: Fail if IBM HTTP Server is still running
      fail:
        msg: "IBM HTTP Server is still running: {{ validate_stop_ihs.stdout }}"
      when: validate_stop_ihs.stdout != ""
Stops IHS and validates using a ps command.

H. handlers/start_dmgr.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/start_dmgr.yml

- name: start_dmgr
  block:
    - name: Gather Deployment Manager status for start
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: dmgr_status_start
      when: is_dmgr | bool
    - name: Check if Deployment Manager is stopped (for start)
      set_fact:
        dmgr_stopped: "{{ (dmgr_status_start.stdout_lines | select('search', 'The Deployment Manager \"dmgr\" is STOPPED') | list | length) > 0 }}"
      when: is_dmgr | bool
    - name: Start the Deployment Manager
      shell: "{{ websphere_install_root }}/profiles/*/bin/startManager.sh"
      args:
        executable: /bin/bash
      when: is_dmgr | bool and dmgr_stopped
    - name: Validate DMGR is started
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: validate_start_dmgr
      changed_when: false
      when: is_dmgr | bool
    - name: Fail if DMGR is not started
      fail:
        msg: "DMGR is not started: {{ validate_start_dmgr.stdout }}"
      when: is_dmgr | bool and ("The Deployment Manager \"dmgr\" is STARTED" not in validate_start_dmgr.stdout)
Starts DMGR and then validates its status.

I. handlers/start_nodeagent.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/start_nodeagent.yml

- name: start_nodeagent
  block:
    - name: Start the NodeAgent
      shell: "{{ websphere_install_root }}/profiles/*/bin/startNode.sh"
      args:
        executable: /bin/bash
    - name: Validate NodeAgent is started
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all | grep -i nodeagent"
      register: validate_start_nodeagent
      changed_when: false
    - name: Fail if NodeAgent is not started
      fail:
        msg: "NodeAgent is not started: {{ validate_start_nodeagent.stdout }}"
      when: validate_start_nodeagent.stdout is not defined or ("is STARTED" not in validate_start_nodeagent.stdout)
Starts NodeAgent and validates that it is running.

J. handlers/start_appservers.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/start_appservers.yml

- name: start_appservers
  block:
    - name: Gather stopped application servers for start
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: ws_status_start
    - name: Set regex for stopped application servers (to start)
      set_fact:
        status_regex: "cannot be reached"
    - name: Filter lines matching stopped application servers
      set_fact:
        target_lines: "{{ ws_status_start.stdout_lines | select('search', status_regex) | list }}"
    - name: Build list of stopped application server names
      set_fact:
        target_servers: >-
          {{
            target_lines
            | map('regex_search', 'The Application Server \"([^\"]+)\"', '\\1')
            | list
          }}
    - name: Order application servers for start (order: ME, then Sup, then App)
      set_fact:
        sorted_servers: >-
          {{
            (target_servers | select('match', '^MEClusterMember') | list) +
            (target_servers | select('match', '^SupClusterMember') | list) +
            (target_servers | select('match', '^AppClusterMember') | list)
          }}
    - name: Set sorted_servers for individual target application server
      set_fact:
        sorted_servers: [ "{{ target_server }}" ]
      when: (target_server | default('')) != ""
    - name: Filter sorted_servers by target group if defined (application servers)
      set_fact:
        sorted_servers: "{{ sorted_servers | select('match', '^' ~ target_group ~ 'ClusterMember') | list }}"
      when: (target_group | default('')) != ""
    - name: Start application servers
      shell: "{{ websphere_install_root }}/profiles/*/bin/startServer.sh {{ server }}"
      loop: "{{ sorted_servers }}"
      loop_control:
        loop_var: server
      when: sorted_servers is defined and (sorted_servers | length > 0)
    - name: Validate application servers are started
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: validate_start_appservers
      changed_when: false
    - name: Fail if application servers are not started
      fail:
        msg: "Application servers are not started as expected: {{ validate_start_appservers.stdout }}"
      when: "'is STARTED' not in validate_start_appservers.stdout"
Starts application servers and then validates that the status output shows they are running.

K. handlers/start_httpserver.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/start_httpserver.yml

- name: start_httpserver
  block:
    - name: Start IBM HTTP Server
      shell: "{{ ihs_install_root }}/bin/apachectl -d {{ ihs_install_root }} -f {{ ihs_install_root }}/conf/{{ ihs_instance }} -k start"
      args:
        executable: /bin/bash
    - name: Validate IBM HTTP Server is started
      shell: "ps -ef | grep httpd | grep '{{ ihs_instance }}'"
      register: validate_start_ihs
      changed_when: false
    - name: Fail if IBM HTTP Server is not started
      fail:
        msg: "IBM HTTP Server is not started: {{ validate_start_ihs.stdout }}"
      when: validate_start_ihs.stdout == ""
Starts IHS and then validates by checking process status.

L. handlers/check_httpserver_status.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/check_httpserver_status.yml

- name: check_httpserver_status
  block:
    - name: Check IBM HTTP Server status using ps command
      shell: "ps -ef | grep httpd | grep '{{ ihs_instance }}'"
      register: ihs_status_check
      changed_when: false
    - name: Debug IBM HTTP Server check status
      debug:
        msg: "IBM HTTP Server check status: {{ ihs_status_check.stdout }}"
This handler is defined for checking IHS status and is not automatically called.

M. handlers/start_httpserver_status.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/start_httpserver_status.yml

- name: start_httpserver_status
  block:
    - name: Check if IBM HTTP Server is running
      shell: "ps -ef | grep httpd | grep '{{ ihs_instance }}'"
      register: ihs_check_status
      changed_when: false
    - name: Debug IBM HTTP Server status before starting
      debug:
        msg: "IBM HTTP Server status: {{ ihs_check_status.stdout }}"
    - name: Start IBM HTTP Server if not running
      shell: "{{ ihs_install_root }}/bin/apachectl -d {{ ihs_install_root }} -f {{ ihs_install_root }}/conf/{{ ihs_instance }} -k start"
      args:
        executable: /bin/bash
      when: ihs_check_status.stdout == ""
This handler checks IHS status and then starts it if it isn’t running (defined but not automatically called).

How to Use This Role
Set Group Variables:

Ensure hosts where the upgrade should run have is_appserver: true.
Set is_dmgr: true on DMGR hosts and is_ihs: true on hosts managing IHS.
Define variables such as:
websphere_install_root (e.g., "/opt/websphere")
ihs_install_root (e.g., "/opt/ihs")
ihs_instance (e.g., "bpm-httpd.conf") in your inventory or group_vars.
Call the Role in Your Playbook:
For example:

yaml
Copy
- hosts: websphere_servers
  roles:
    - role: upgrade_websphere
      vars:
        kill_timeout: 1500
        websphere_install_root: "/opt/websphere"
        ihs_install_root: "/opt/ihs"
        ihs_instance: "bpm-httpd.conf"
Upgrade Process Flow:

The main tasks notify the stop handlers (which include validation steps) to shut down application servers, NodeAgent, DMGR, and IHS (if applicable).
Your upgrade/validation commands run.
Then, the start handlers are notified to start DMGR, NodeAgent, application servers, and IHS (with validations to ensure each service is up).
Two extra handlers (check_httpserver_status.yml and start_httpserver_status.yml) are available if you wish to add additional IHS status checking steps.
This role now integrates validation tasks directly into the stop and start handler files. If any service does not reach the desired state (stopped or started), the handler will fail, ensuring that the upgrade process does not continue without all services being in the expected state.






You said:
make sure the above role is runnable in ansible 2.6
Reasoned for a few seconds
Below is the final version of the upgrade_websphere role. This version has been reviewed for compatibility with Ansible 2.6. All features used (such as import_tasks, loop_control, regex_search, and when conditions with the bool filter) are supported in Ansible 2.6.

Below you’ll find the complete role with variable-based paths, integrated validation steps in the stop/start handlers, and additional subtasks for stopping/starting the IBM HTTP Server (IHS). (Note that the two extra handlers—check_httpserver_status.yml and start_httpserver_status.yml—are defined but not automatically called.)

You can copy the following directory structure and file contents into your Ansible roles directory and run the role with Ansible 2.6.

Directory Structure
css
Copy
roles/
└── upgrade_websphere
    ├── defaults
    │   └── main.yml
    ├── tasks
    │   └── main.yml
    └── handlers
        ├── main.yml
        ├── stop_appservers.yml
        ├── stop_nodeagent.yml
        ├── stop_dmgr.yml
        ├── stop_httpserver.yml
        ├── start_dmgr.yml
        ├── start_nodeagent.yml
        ├── start_appservers.yml
        ├── start_httpserver.yml
        ├── check_httpserver_status.yml
        └── start_httpserver_status.yml
1. defaults/main.yml
yaml
Copy
---
# roles/upgrade_websphere/defaults/main.yml

# Default kill timeout in seconds (e.g., 1200 = 20 minutes)
kill_timeout: 1200

# Optional filters for application servers; leave empty to operate on all.
target_server: ""
target_group: ""
This file defines default variables used across the role.

2. tasks/main.yml
yaml
Copy
---
# roles/upgrade_websphere/tasks/main.yml

- block:
    - name: Stop all services
      debug:
        msg: "Notifying handlers to stop application servers, NodeAgent and DMGR..."
      notify:
        - stop_appservers
        - stop_nodeagent
        - stop_dmgr

    - name: Stop IBM HTTP Server
      debug:
        msg: "Notifying handler to stop IBM HTTP Server..."
      notify:
        - stop_httpserver
      when: is_ihs | bool

    - name: Perform upgrade and validation tasks
      debug:
        msg: "Performing upgrade tasks and validations..."
      # Replace this with your actual upgrade/validation commands

    - name: Start all services
      debug:
        msg: "Notifying handlers to start DMGR, NodeAgent and application servers..."
      notify:
        - start_dmgr
        - start_nodeagent
        - start_appservers

    - name: Start IBM HTTP Server
      debug:
        msg: "Notifying handler to start IBM HTTP Server..."
      notify:
        - start_httpserver
      when: is_ihs | bool

  when: is_appserver | bool
The main tasks file notifies the stop handlers before the upgrade and the start handlers after your upgrade tasks. (Optional validation steps are integrated into the handlers.)

3. handlers/main.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/main.yml

- import_tasks: stop_appservers.yml
- import_tasks: stop_nodeagent.yml
- import_tasks: stop_dmgr.yml
- import_tasks: stop_httpserver.yml
- import_tasks: start_dmgr.yml
- import_tasks: start_nodeagent.yml
- import_tasks: start_appservers.yml
- import_tasks: start_httpserver.yml
- import_tasks: check_httpserver_status.yml
- import_tasks: start_httpserver_status.yml
This file imports all individual handler files.

4. handlers/stop_appservers.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/stop_appservers.yml

- name: stop_appservers
  block:
    - name: Gather running application servers for stop
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: ws_status_stop
      when: (target_server | default('')) == "" and (target_group | default('')) == ""

    - name: Set regex for running application servers (to stop)
      set_fact:
        status_regex: "is STARTED"
      when: (target_server | default('')) == "" and (target_group | default('')) == ""

    - name: Filter lines matching running application servers
      set_fact:
        target_lines: "{{ ws_status_stop.stdout_lines | select('search', status_regex) | list }}"
      when: (target_server | default('')) == "" and (target_group | default('')) == ""

    - name: Build list of running application server names
      set_fact:
        target_servers: >-
          {{
            target_lines
            | map('regex_search', 'The Application Server \"([^\"]+)\"', '\\1')
            | list
          }}
      when: (target_server | default('')) == "" and (target_group | default('')) == ""

    - name: Order application servers for stop (reverse order: App, then Sup, then ME)
      set_fact:
        sorted_servers: >-
          {{
            (target_servers | select('match', '^AppClusterMember') | list) +
            (target_servers | select('match', '^SupClusterMember') | list) +
            (target_servers | select('match', '^MEClusterMember') | list)
          }}
      when: (target_server | default('')) == "" and (target_group | default('')) == ""

    - name: Set sorted_servers for individual target application server
      set_fact:
        sorted_servers: [ "{{ target_server }}" ]
      when: (target_server | default('')) != ""

    - name: Filter sorted_servers by target group if defined (application servers)
      set_fact:
        sorted_servers: "{{ sorted_servers | select('match', '^' ~ target_group ~ 'ClusterMember') | list }}"
      when: (target_group | default('')) != ""

    - name: Stop application servers
      shell: |
        {{ websphere_install_root }}/profiles/*/bin/stopServer.sh {{ server }}
        counter=0
        while ps -ef | grep -v grep | grep -q "{{ server }}"; do
          sleep 10
          counter=$((counter+10))
          if [ $counter -ge {{ kill_timeout }} ]; then
            echo "Application server {{ server }} still running after {{ kill_timeout }} seconds. Killing process..."
            ps -ef | grep -v grep | grep "{{ server }}" | awk '{print $2}' | xargs kill -9 || true
            break
          fi
        done
      args:
        executable: /bin/bash
      loop: "{{ sorted_servers }}"
      loop_control:
        loop_var: server
      when: sorted_servers is defined and (sorted_servers | length > 0)

    - name: Validate application servers are stopped
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: validate_stop_appservers
      changed_when: false

    - name: Fail if any application server is still running
      fail:
        msg: "Application servers are still running: {{ validate_stop_appservers.stdout }}"
      when: "'is STARTED' in validate_stop_appservers.stdout"
After stopping the application servers, the handler validates that none remain running.

5. handlers/stop_nodeagent.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/stop_nodeagent.yml

- name: stop_nodeagent
  block:
    - name: Stop the NodeAgent
      shell: "{{ websphere_install_root }}/profiles/*/bin/stopNode.sh"
      args:
        executable: /bin/bash

    - name: Validate NodeAgent is stopped
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all | grep -i nodeagent"
      register: validate_stop_nodeagent
      changed_when: false

    - name: Fail if NodeAgent is still running
      fail:
        msg: "NodeAgent is still running: {{ validate_stop_nodeagent.stdout }}"
      when: validate_stop_nodeagent.stdout is defined and ("is STARTED" in validate_stop_nodeagent.stdout)
This handler stops the NodeAgent and validates its shutdown.

6. handlers/stop_dmgr.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/stop_dmgr.yml

- name: stop_dmgr
  block:
    - name: Gather Deployment Manager status for stop
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: dmgr_status_stop
      when: is_dmgr | bool

    - name: Check if Deployment Manager is running (for stop)
      set_fact:
        dmgr_running: "{{ (dmgr_status_stop.stdout_lines | select('search', 'The Deployment Manager \"dmgr\" is STARTED') | list | length) > 0 }}"
      when: is_dmgr | bool

    - name: Stop the Deployment Manager
      shell: "{{ websphere_install_root }}/profiles/*/bin/stopManager.sh"
      args:
        executable: /bin/bash
      when: is_dmgr | bool and dmgr_running

    - name: Validate DMGR is stopped
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: validate_stop_dmgr
      changed_when: false
      when: is_dmgr | bool

    - name: Fail if DMGR is still running
      fail:
        msg: "DMGR is still running: {{ validate_stop_dmgr.stdout }}"
      when: is_dmgr | bool and ("The Deployment Manager \"dmgr\" is STARTED" in validate_stop_dmgr.stdout)
Stops DMGR (if applicable) and validates that it is down.

7. handlers/stop_httpserver.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/stop_httpserver.yml

- name: stop_httpserver
  block:
    - name: Stop IBM HTTP Server
      shell: "{{ ihs_install_root }}/bin/apachectl -d {{ ihs_install_root }} -f {{ ihs_install_root }}/conf/{{ ihs_instance }} -k stop"
      args:
        executable: /bin/bash

    - name: Validate IBM HTTP Server is stopped
      shell: "ps -ef | grep httpd | grep '{{ ihs_instance }}'"
      register: validate_stop_ihs
      changed_when: false

    - name: Fail if IBM HTTP Server is still running
      fail:
        msg: "IBM HTTP Server is still running: {{ validate_stop_ihs.stdout }}"
      when: validate_stop_ihs.stdout != ""
Stops IHS and validates that the process is not running.

8. handlers/start_dmgr.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/start_dmgr.yml

- name: start_dmgr
  block:
    - name: Gather Deployment Manager status for start
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: dmgr_status_start
      when: is_dmgr | bool

    - name: Check if Deployment Manager is stopped (for start)
      set_fact:
        dmgr_stopped: "{{ (dmgr_status_start.stdout_lines | select('search', 'The Deployment Manager \"dmgr\" is STOPPED') | list | length) > 0 }}"
      when: is_dmgr | bool

    - name: Start the Deployment Manager
      shell: "{{ websphere_install_root }}/profiles/*/bin/startManager.sh"
      args:
        executable: /bin/bash
      when: is_dmgr | bool and dmgr_stopped

    - name: Validate DMGR is started
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: validate_start_dmgr
      changed_when: false
      when: is_dmgr | bool

    - name: Fail if DMGR is not started
      fail:
        msg: "DMGR is not started: {{ validate_start_dmgr.stdout }}"
      when: is_dmgr | bool and ("The Deployment Manager \"dmgr\" is STARTED" not in validate_start_dmgr.stdout)
Starts DMGR and validates that it is up.

9. handlers/start_nodeagent.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/start_nodeagent.yml

- name: start_nodeagent
  block:
    - name: Start the NodeAgent
      shell: "{{ websphere_install_root }}/profiles/*/bin/startNode.sh"
      args:
        executable: /bin/bash

    - name: Validate NodeAgent is started
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all | grep -i nodeagent"
      register: validate_start_nodeagent
      changed_when: false

    - name: Fail if NodeAgent is not started
      fail:
        msg: "NodeAgent is not started: {{ validate_start_nodeagent.stdout }}"
      when: validate_start_nodeagent.stdout is not defined or ("is STARTED" not in validate_start_nodeagent.stdout)
Starts NodeAgent and validates that it is running.

10. handlers/start_appservers.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/start_appservers.yml

- name: start_appservers
  block:
    - name: Gather stopped application servers for start
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: ws_status_start

    - name: Set regex for stopped application servers (to start)
      set_fact:
        status_regex: "cannot be reached"

    - name: Filter lines matching stopped application servers
      set_fact:
        target_lines: "{{ ws_status_start.stdout_lines | select('search', status_regex) | list }}"

    - name: Build list of stopped application server names
      set_fact:
        target_servers: >-
          {{
            target_lines
            | map('regex_search', 'The Application Server \"([^\"]+)\"', '\\1')
            | list
          }}

    - name: Order application servers for start (order: ME, then Sup, then App)
      set_fact:
        sorted_servers: >-
          {{
            (target_servers | select('match', '^MEClusterMember') | list) +
            (target_servers | select('match', '^SupClusterMember') | list) +
            (target_servers | select('match', '^AppClusterMember') | list)
          }}

    - name: Set sorted_servers for individual target application server
      set_fact:
        sorted_servers: [ "{{ target_server }}" ]
      when: (target_server | default('')) != ""

    - name: Filter sorted_servers by target group if defined (application servers)
      set_fact:
        sorted_servers: "{{ sorted_servers | select('match', '^' ~ target_group ~ 'ClusterMember') | list }}"
      when: (target_group | default('')) != ""

    - name: Start application servers
      shell: "{{ websphere_install_root }}/profiles/*/bin/startServer.sh {{ server }}"
      loop: "{{ sorted_servers }}"
      loop_control:
        loop_var: server
      when: sorted_servers is defined and (sorted_servers | length > 0)

    - name: Validate application servers are started
      shell: "{{ websphere_install_root }}/profiles/*/bin/serverStatus.sh -all"
      register: validate_start_appservers
      changed_when: false

    - name: Fail if application servers are not started
      fail:
        msg: "Application servers are not started as expected: {{ validate_start_appservers.stdout }}"
      when: "'is STARTED' not in validate_start_appservers.stdout"
Starts application servers and validates that they are running.

11. handlers/start_httpserver.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/start_httpserver.yml

- name: start_httpserver
  block:
    - name: Start IBM HTTP Server
      shell: "{{ ihs_install_root }}/bin/apachectl -d {{ ihs_install_root }} -f {{ ihs_install_root }}/conf/{{ ihs_instance }} -k start"
      args:
        executable: /bin/bash

    - name: Validate IBM HTTP Server is started
      shell: "ps -ef | grep httpd | grep '{{ ihs_instance }}'"
      register: validate_start_ihs
      changed_when: false

    - name: Fail if IBM HTTP Server is not started
      fail:
        msg: "IBM HTTP Server is not started: {{ validate_start_ihs.stdout }}"
      when: validate_start_ihs.stdout == ""
Starts IHS and validates that it is running by checking process output.

12. handlers/check_httpserver_status.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/check_httpserver_status.yml

- name: check_httpserver_status
  block:
    - name: Check IBM HTTP Server status using ps command
      shell: "ps -ef | grep httpd | grep '{{ ihs_instance }}'"
      register: ihs_status_check
      changed_when: false

    - name: Debug IBM HTTP Server check status
      debug:
        msg: "IBM HTTP Server check status: {{ ihs_status_check.stdout }}"
This handler checks the IHS status (it is defined but not automatically called).

13. handlers/start_httpserver_status.yml
yaml
Copy
---
# roles/upgrade_websphere/handlers/start_httpserver_status.yml

- name: start_httpserver_status
  block:
    - name: Check if IBM HTTP Server is running
      shell: "ps -ef | grep httpd | grep '{{ ihs_instance }}'"
      register: ihs_check_status
      changed_when: false

    - name: Debug IBM HTTP Server status before starting
      debug:
        msg: "IBM HTTP Server status: {{ ihs_check_status.stdout }}"

    - name: Start IBM HTTP Server if not running
      shell: "{{ ihs_install_root }}/bin/apachectl -d {{ ihs_install_root }} -f {{ ihs_install_root }}/conf/{{ ihs_instance }} -k start"
      args:
        executable: /bin/bash
      when: ihs_check_status.stdout == ""
This handler checks IHS status and then starts it if not running (defined but not automatically called).

How to Use This Role
Set Group Variables:

For hosts where the upgrade should run, set is_appserver: true.
For DMGR hosts, set is_dmgr: true.
For hosts managing IHS, set is_ihs: true.
Define variables such as:
websphere_install_root (e.g., "/opt/websphere")
ihs_install_root (e.g., "/opt/ihs")
ihs_instance (e.g., "bpm-httpd.conf") in your inventory or group_vars.
Call the Role in Your Playbook:
For example:

yaml
Copy
- hosts: websphere_servers
  roles:
    - role: upgrade_websphere
      vars:
        kill_timeout: 1500        # Optional override
        websphere_install_root: "/opt/websphere"
        ihs_install_root: "/opt/ihs"
        ihs_instance: "bpm-httpd.conf"
Validation:
The stop and start handlers automatically validate that the services are in the expected state (down or up). If a service does not reach the desired state, the handler will fail, stopping the play.

Extra IHS Handlers:
The extra handlers check_httpserver_status.yml and start_httpserver_status.yml are defined and available for use if you need additional IHS status checking but are not automatically called by the role.

This role has been structured to run on Ansible 2.6 without issues. All tasks, handlers, and filters used are supported by Ansible 2.6, so you should be able to run this role in your environment with that version.

Feel free to customize the upgrade tasks and validation commands as needed for your specific environment.
